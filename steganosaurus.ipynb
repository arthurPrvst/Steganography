{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"*** \" + torch.cuda.get_device_name(torch.cuda.current_device()) + \" ***\")\n",
    "else:\n",
    "    print(\"WARNING : CUDA NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(im, size, N):\n",
    "    I = torch.ones(im.size(0)+1-size[0]).multinomial(N,replacement=True)\n",
    "    J = torch.ones(im.size(1)+1-size[1]).multinomial(N,replacement=True)\n",
    "    out = torch.FloatTensor(N, size[0],size[1]).zero_()\n",
    "    for k,(i,j) in enumerate(zip(I,J)):\n",
    "        out[k] = im[i:i+size[0],j:j+size[1]]\n",
    "    return(out)\n",
    "\n",
    "Faces = []\n",
    "src = '/home/pphan/Downloads/yalefaces/'\n",
    "for file in os.scandir(src):\n",
    "    im = torch.from_numpy(plt.imread(file.path))\n",
    "    Faces.append(crop(im,(232,232),20).div(255))\n",
    "Faces = F.upsample(torch.cat(Faces).unsqueeze(1), size=128, mode='bilinear').data\n",
    "print(Faces.size())\n",
    "\n",
    "Text = []\n",
    "src = '/home/pphan/Downloads/Projet_Steganographie/generated_secret'\n",
    "for file in os.scandir(src):\n",
    "    im = torch.from_numpy(plt.imread(file.path))\n",
    "    Text.append(im.mean(dim=2))\n",
    "Text = torch.stack(Text).unsqueeze(1)\n",
    "print(Text.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels, down=True):\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        self.inp = inp_channels\n",
    "        self.out = out_channels\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.inp, self.out, 3, padding=1, bias=False),\n",
    "            nn.LeakyReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.out, self.out, 3, stride=down+1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.out), nn.LeakyReLU())\n",
    "\n",
    "    def forward(self, X):\n",
    "        c = self.conv1(X)\n",
    "        return(c, self.conv2(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class deconv_block(nn.Module):\n",
    "    def __init__(self, inp_channels, out_channels):\n",
    "        super(deconv_block, self).__init__()\n",
    "\n",
    "        self.inp = inp_channels\n",
    "        self.out = out_channels\n",
    "        \n",
    "        self.upconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.inp, self.out, 2, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(self.out), nn.ReLU())\n",
    "\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.inp, self.out, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.out), nn.ReLU())\n",
    "\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.out, self.out, 3, padding=1, bias=False),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, X, feature):\n",
    "        return(self.deconv2(self.deconv1(torch.cat((feature,self.upconv(X)), dim=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class contract(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(contract, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(1,64)\n",
    "        self.conv2 = conv_block(64,128)\n",
    "        self.conv3 = conv_block(128,256)\n",
    "        self.conv4 = conv_block(256,512, down=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        f0,X = self.conv1(X) #(64, 128, 128),(64, 64, 64)\n",
    "        f1,X = self.conv2(X) #(128, 64, 64),(128, 32, 32)\n",
    "        f2,X = self.conv3(X) #(256, 32, 32),(256, 16, 16)\n",
    "        _,X = self.conv4(X) #(512, 16, 16)\n",
    "        return(X, (f0,f1,f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class expand(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(expand, self).__init__()\n",
    "        \n",
    "        self.deconv1 = deconv_block(512,256)\n",
    "        self.deconv2 = deconv_block(256,128)\n",
    "        self.deconv3 = deconv_block(128,64)\n",
    "        self.deconv4 = nn.Sequential(nn.Conv2d(64, 1, 1), nn.Tanh())\n",
    "    \n",
    "    def forward(self, X, features):\n",
    "        return(0.5+0.5*self.deconv4(self.deconv3(self.deconv2(self.deconv1(X, features[2]), features[1]), features[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encryptor_decryptor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encryptor_decryptor, self).__init__()\n",
    "        \n",
    "        self.contract_source = contract()\n",
    "        self.contract_message = contract()\n",
    "        self.mix = nn.Sequential(nn.Conv2d(1024, 512, 3, padding=1), nn.LeakyReLU())\n",
    "        self.expand_source = expand()\n",
    "        \n",
    "        self.contract_output = contract()\n",
    "        self.expand_output = expand()\n",
    "    \n",
    "    def forward(self, source, message):\n",
    "        source,features = self.contract_source(source)\n",
    "        message,_ = self.contract_message(message)\n",
    "        hidden = self.expand_source(self.mix(torch.cat((source,message), dim=1)), features)\n",
    "        \n",
    "        output,features = self.contract_output(hidden)\n",
    "        return(hidden, self.expand_output(output, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, batch_size=8):\n",
    "        self.num_layers = 0\n",
    "        self.num_params = 0\n",
    "        \n",
    "        self.model = encryptor_decryptor().cuda()\n",
    "        for p in self.model.parameters():\n",
    "            self.num_params += p.view(-1).size(0)\n",
    "        self.model.apply(self.init_weights)\n",
    "#         self.model.load_state_dict(torch.load('saved_models/gan/truc.model'))\n",
    "        print(\"# layers : {}, # params : {}\".format(self.num_layers,self.num_params))\n",
    "        \n",
    "        self.optim = torch.optim.Adam(self.model.parameters(), lr=0.0002, betas=(0.5,0.999))\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.times = []\n",
    "        self.enc_losses_min = []\n",
    "        self.enc_losses_max = []\n",
    "        self.dec_losses_min = []\n",
    "        self.dec_losses_max = []\n",
    "        \n",
    "    def init_weights(self,m):\n",
    "        G_leaky = nn.init.calculate_gain('leaky_relu')\n",
    "        G_relu = nn.init.calculate_gain('relu')\n",
    "        G_tanh = nn.init.calculate_gain('tanh')\n",
    "        if type(m)==nn.Conv2d:\n",
    "            if m.out_channels == 1:\n",
    "                nn.init.xavier_normal(m.weight, gain=G_tanh)\n",
    "                self.num_layers += 1\n",
    "            else:\n",
    "                nn.init.xavier_normal(m.weight, gain=G_leaky)\n",
    "                self.num_layers += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant(m.bias, val=0)\n",
    "        elif type(m)==nn.ConvTranspose2d:\n",
    "            nn.init.xavier_normal(m.weight, gain=G_relu)\n",
    "            self.num_layers += 1\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant(m.bias, val=0)\n",
    "    \n",
    "    def new_epoch(self):\n",
    "        self.epoch += 1\n",
    "        if self.epoch%10==0:\n",
    "            torch.save(self.model.state_dict(),'saved_models/gan/steganosaurus_e{}.model'.format(self.epoch))\n",
    "    \n",
    "    def add_losses(self, losses, t):\n",
    "        self.times.append(self.epoch + t)\n",
    "        self.enc_losses_min.append(min(losses[0]))\n",
    "        self.enc_losses_max.append(max(losses[0]))\n",
    "        self.dec_losses_min.append(min(losses[1]))\n",
    "        self.dec_losses_max.append(max(losses[1]))\n",
    "    \n",
    "    def disp(self, display):\n",
    "        if display is not None:\n",
    "            display[0].clear()\n",
    "            display[0].plot(self.times,self.enc_losses_min,'r')\n",
    "            display[0].plot(self.times,self.enc_losses_max,'r')\n",
    "            display[0].plot(self.times,self.dec_losses_min,'b')\n",
    "            display[0].plot(self.times,self.dec_losses_max,'b')\n",
    "            display[1].canvas.draw()\n",
    "    \n",
    "    def draw(self, display, data):\n",
    "        if display is not None:   \n",
    "            for k,ax in enumerate(display[0]):\n",
    "                ax.clear()\n",
    "                ax.imshow(data[0][k].cpu().numpy(), cmap='gray')\n",
    "            for k,ax in enumerate(display[1]):\n",
    "                ax.clear()\n",
    "                ax.imshow(data[1][k].cpu().numpy(), cmap='gray')\n",
    "            for k,ax in enumerate(display[2]):\n",
    "                ax.clear()\n",
    "                ax.imshow(data[2][k].cpu().numpy(), cmap='gray')\n",
    "            for k,ax in enumerate(display[3]):\n",
    "                ax.clear()\n",
    "                ax.imshow(data[3][k].cpu().numpy(), cmap='gray')\n",
    "            display[4].canvas.draw()\n",
    "\n",
    "    def train(self, max_epochs=500, points_per_epoch=1, display=None):\n",
    "        eps = 1e-7\n",
    "        points = [int(n)-1 for n in torch.linspace(0,Text.size(0)//self.batch_size,1+points_per_epoch)[1:]]\n",
    "        self.model.train()\n",
    "        \n",
    "        ep = 0\n",
    "        keep = True\n",
    "        enc_losses,dec_losses = [],[]\n",
    "        while keep and ep<max_epochs:\n",
    "            try:\n",
    "                self.new_epoch()\n",
    "                ep += 1\n",
    "                shuffle_f = torch.randperm(Text.size(0))\n",
    "                shuffle_t = torch.randperm(Text.size(0))\n",
    "                for mb in range(Text.size(0)//self.batch_size):\n",
    "                    \n",
    "                    source = Variable(Faces[shuffle_f[mb*self.batch_size:(mb+1)*self.batch_size]].cuda(), requires_grad=False)\n",
    "                    message = Variable(Faces[shuffle_t[mb*self.batch_size:(mb+1)*self.batch_size]].cuda(), requires_grad=False)\n",
    "                    hidden,output = self.model(source, message)\n",
    "                    \n",
    "                    lossE = F.mse_loss(hidden,source)\n",
    "                    lossD = F.mse_loss(output,message)\n",
    "                    alpha = 0.8\n",
    "                    loss = alpha*lossE + (1-alpha)*lossD\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "\n",
    "                    enc_losses.append(lossE.data[0])\n",
    "                    dec_losses.append(lossD.data[0])\n",
    "                    \n",
    "                    if mb in points:\n",
    "                        self.add_losses((enc_losses,dec_losses),\n",
    "                                        float(mb/(Text.size(0)//self.batch_size))-1)\n",
    "                        self.model.eval()\n",
    "                        source = Variable(Faces[shuffle_f[mb*self.batch_size:mb*self.batch_size+3]].cuda(), requires_grad=False)\n",
    "                        message = Variable(Faces[shuffle_t[mb*self.batch_size:mb*self.batch_size+3]].cuda(), requires_grad=False)\n",
    "                        hidden,output = self.model(source, message)\n",
    "                        self.draw((display[1],display[2],display[3],display[4],display[0]),\n",
    "                                  (message.data.squeeze(),source.data.squeeze(),\n",
    "                                   hidden.data.squeeze(),output.data.squeeze(),))\n",
    "                        self.model.train()\n",
    "                        \n",
    "                        self.disp((display[5],display[0]))\n",
    "                        enc_losses,dec_losses = [],[]\n",
    "                                \n",
    "            except KeyboardInterrupt:\n",
    "                keep = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "trainer = Trainer(batch_size=16)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "msg = [fig.add_subplot(4,4,k) for k in [1,5,9]]\n",
    "src = [fig.add_subplot(4,4,k) for k in [2,6,10]]\n",
    "hid = [fig.add_subplot(4,4,k) for k in [3,7,11]]\n",
    "out = [fig.add_subplot(4,4,k) for k in [4,8,12]]\n",
    "board = fig.add_subplot(4,1,4)\n",
    "plt.ion()\n",
    "fig.show()\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(points_per_epoch=1, display=(fig,msg,src,hid,out,board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
